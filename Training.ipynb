{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters \n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines [:min(num_samples,len(lines)-1)]:\n",
    "    input_text, target_text, _ = line.split('\\t') # to split the input text and  the target a\n",
    "    target_text =  '\\t' + target_text + '\\n' \n",
    "    input_texts.append(input_text.lower())\n",
    "    target_texts.append(target_text.lower())\n",
    "    for char in input_text.lower():\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text.lower():\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters,\n",
    "             'max_input_length':max_encoder_seq_length,'max_target_length':max_decoder_seq_length,\n",
    "             'num_en_chars':num_encoder_tokens,'num_dec_chars':num_decoder_tokens},open(\"training_data.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_texts,target_texts):\n",
    "  #inintialize encoder , decoder input and target data.\n",
    "  en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "  #padding variable with first character as 1 as rest all 0.\n",
    "  pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "  pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "  #countvectorizer for one hot encoding as we want to tokenize character so\n",
    "  #anlyzer is true and None the stopwords action.\n",
    "  cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')\n",
    "  for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "    #fit the input characters into the CountVectorizer function\n",
    "    cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "    #transform the input text from the help of CountVectorizer fit.\n",
    "    #it character present than put 1 and 0 otherwise.\n",
    "    en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "    cv_tar= cv.fit(target_characters)\t\t\n",
    "    dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "    #decoder target will be one timestep ahead because it will not consider \n",
    "    #the first character i.e. '\\t'.\n",
    "    dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "    \n",
    "    #add padding variable if the length of the input or target text is smaller\n",
    "    #than their respective maximum input or target length. \n",
    "    if len(input_t) < max_encoder_seq_length:\n",
    "      for _ in range(max_encoder_seq_length-len(input_t)):\n",
    "        en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_decoder_seq_length:\n",
    "      for _ in range(max_decoder_seq_length-len(target_t)):\n",
    "        dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_decoder_seq_length:\n",
    "      for _ in range(max_decoder_seq_length-len(target_t)+1):\n",
    "        dec_tr_data[i].append(pad_dec)\n",
    "  \n",
    "  #convert list to numpy array with data type float32\n",
    "  en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "  dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "  dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "\n",
    "  return en_in_data,dec_in_data,dec_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, None, 47)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None, 65)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 256), (None, 311296      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  329728      input_4[0][0]                    \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 65)     16705       lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 657,729\n",
      "Trainable params: 657,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state = True)\n",
    "decoder_outputs,_,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation= 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data,decoder_input_data,decoder_target_data = bagofcharacters(input_texts,target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 1.2775 - accuracy: 0.7209 - val_loss: 1.0733 - val_accuracy: 0.6993\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 54s 431ms/step - loss: 0.8817 - accuracy: 0.7551 - val_loss: 0.9012 - val_accuracy: 0.7495\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.7969 - accuracy: 0.7855 - val_loss: 0.8288 - val_accuracy: 0.7665\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 52s 413ms/step - loss: 0.6751 - accuracy: 0.8077 - val_loss: 0.7353 - val_accuracy: 0.7869\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.6192 - accuracy: 0.8207 - val_loss: 0.6901 - val_accuracy: 0.7996\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.5850 - accuracy: 0.8299 - val_loss: 0.6580 - val_accuracy: 0.8066\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 0.5581 - accuracy: 0.8365 - val_loss: 0.6354 - val_accuracy: 0.8132\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 57s 456ms/step - loss: 0.5365 - accuracy: 0.8430 - val_loss: 0.6169 - val_accuracy: 0.8192\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 60s 480ms/step - loss: 0.5218 - accuracy: 0.8464 - val_loss: 0.5968 - val_accuracy: 0.8249\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 58s 462ms/step - loss: 0.5020 - accuracy: 0.8521 - val_loss: 0.5838 - val_accuracy: 0.8271\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 57s 453ms/step - loss: 0.4876 - accuracy: 0.8559 - val_loss: 0.5694 - val_accuracy: 0.8321\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 61s 487ms/step - loss: 0.4737 - accuracy: 0.8601 - val_loss: 0.5567 - val_accuracy: 0.8346\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 56s 447ms/step - loss: 0.4610 - accuracy: 0.8635 - val_loss: 0.5447 - val_accuracy: 0.8396\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 51s 407ms/step - loss: 0.4499 - accuracy: 0.8667 - val_loss: 0.5340 - val_accuracy: 0.8413\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 59s 475ms/step - loss: 0.4377 - accuracy: 0.8699 - val_loss: 0.5253 - val_accuracy: 0.8441\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 57s 456ms/step - loss: 0.4274 - accuracy: 0.8729 - val_loss: 0.5145 - val_accuracy: 0.8472\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 57s 454ms/step - loss: 0.4162 - accuracy: 0.8759 - val_loss: 0.5057 - val_accuracy: 0.8503\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 0.4067 - accuracy: 0.8788 - val_loss: 0.4946 - val_accuracy: 0.8535\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 56s 451ms/step - loss: 0.3962 - accuracy: 0.8814 - val_loss: 0.4903 - val_accuracy: 0.8553\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 57s 454ms/step - loss: 0.3873 - accuracy: 0.8839 - val_loss: 0.4869 - val_accuracy: 0.8556\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.3787 - accuracy: 0.8867 - val_loss: 0.4795 - val_accuracy: 0.8573\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.3687 - accuracy: 0.8895 - val_loss: 0.4727 - val_accuracy: 0.8590\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 53s 423ms/step - loss: 0.3599 - accuracy: 0.8920 - val_loss: 0.4654 - val_accuracy: 0.8617\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.3509 - accuracy: 0.8944 - val_loss: 0.4588 - val_accuracy: 0.8629\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 51s 411ms/step - loss: 0.3423 - accuracy: 0.8969 - val_loss: 0.4560 - val_accuracy: 0.8642\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 51s 411ms/step - loss: 0.3347 - accuracy: 0.8989 - val_loss: 0.4515 - val_accuracy: 0.8652\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.3268 - accuracy: 0.9015 - val_loss: 0.4479 - val_accuracy: 0.8669\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 0.3182 - accuracy: 0.9038 - val_loss: 0.4465 - val_accuracy: 0.8679\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.3103 - accuracy: 0.9063 - val_loss: 0.4454 - val_accuracy: 0.8677\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.3037 - accuracy: 0.9081 - val_loss: 0.4401 - val_accuracy: 0.8701\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 0.2957 - accuracy: 0.9107 - val_loss: 0.4375 - val_accuracy: 0.8714\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.2889 - accuracy: 0.9126 - val_loss: 0.4390 - val_accuracy: 0.8711\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 51s 412ms/step - loss: 0.2810 - accuracy: 0.9150 - val_loss: 0.4334 - val_accuracy: 0.8725\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.2736 - accuracy: 0.9170 - val_loss: 0.4335 - val_accuracy: 0.8727\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 51s 404ms/step - loss: 0.2669 - accuracy: 0.9192 - val_loss: 0.4350 - val_accuracy: 0.8734\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.2598 - accuracy: 0.9211 - val_loss: 0.4358 - val_accuracy: 0.8738\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.2534 - accuracy: 0.9232 - val_loss: 0.4355 - val_accuracy: 0.8736\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.2466 - accuracy: 0.9248 - val_loss: 0.4342 - val_accuracy: 0.8749\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.2406 - accuracy: 0.9267 - val_loss: 0.4347 - val_accuracy: 0.8755\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 52s 416ms/step - loss: 0.2346 - accuracy: 0.9288 - val_loss: 0.4368 - val_accuracy: 0.8751\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.2276 - accuracy: 0.9307 - val_loss: 0.4391 - val_accuracy: 0.8752\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.2217 - accuracy: 0.9323 - val_loss: 0.4387 - val_accuracy: 0.8757\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 50s 404ms/step - loss: 0.2154 - accuracy: 0.9343 - val_loss: 0.4400 - val_accuracy: 0.8764\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.2093 - accuracy: 0.9362 - val_loss: 0.4431 - val_accuracy: 0.8768\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 52s 412ms/step - loss: 0.2042 - accuracy: 0.9378 - val_loss: 0.4432 - val_accuracy: 0.8766\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 53s 421ms/step - loss: 0.1983 - accuracy: 0.9395 - val_loss: 0.4462 - val_accuracy: 0.8769\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.1940 - accuracy: 0.9408 - val_loss: 0.4510 - val_accuracy: 0.8759\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.1882 - accuracy: 0.9425 - val_loss: 0.4483 - val_accuracy: 0.8777\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.1833 - accuracy: 0.9443 - val_loss: 0.4527 - val_accuracy: 0.8775\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 51s 408ms/step - loss: 0.1774 - accuracy: 0.9457 - val_loss: 0.4568 - val_accuracy: 0.8765\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 51s 404ms/step - loss: 0.1737 - accuracy: 0.9467 - val_loss: 0.4627 - val_accuracy: 0.8767\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.1691 - accuracy: 0.9483 - val_loss: 0.4650 - val_accuracy: 0.8765\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.1645 - accuracy: 0.9495 - val_loss: 0.4717 - val_accuracy: 0.8762\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 53s 422ms/step - loss: 0.1603 - accuracy: 0.9511 - val_loss: 0.4698 - val_accuracy: 0.8766\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 54s 432ms/step - loss: 0.1546 - accuracy: 0.9527 - val_loss: 0.4761 - val_accuracy: 0.8766\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.1512 - accuracy: 0.9535 - val_loss: 0.4808 - val_accuracy: 0.8761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 51s 409ms/step - loss: 0.1466 - accuracy: 0.9552 - val_loss: 0.4858 - val_accuracy: 0.8766\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1442 - accuracy: 0.9558 - val_loss: 0.4893 - val_accuracy: 0.8764\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.1399 - accuracy: 0.9568 - val_loss: 0.4908 - val_accuracy: 0.8766\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.1353 - accuracy: 0.9587 - val_loss: 0.4969 - val_accuracy: 0.8760\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1319 - accuracy: 0.9594 - val_loss: 0.4963 - val_accuracy: 0.8769\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.1292 - accuracy: 0.9600 - val_loss: 0.5055 - val_accuracy: 0.8748\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.1255 - accuracy: 0.9614 - val_loss: 0.5101 - val_accuracy: 0.8758\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 50s 400ms/step - loss: 0.1220 - accuracy: 0.9623 - val_loss: 0.5142 - val_accuracy: 0.8750\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 51s 408ms/step - loss: 0.1248 - accuracy: 0.9611 - val_loss: 0.5197 - val_accuracy: 0.8744\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1191 - accuracy: 0.9628 - val_loss: 0.5236 - val_accuracy: 0.8754\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.1135 - accuracy: 0.9648 - val_loss: 0.5236 - val_accuracy: 0.8751\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.1103 - accuracy: 0.9657 - val_loss: 0.5308 - val_accuracy: 0.8756\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.1075 - accuracy: 0.9664 - val_loss: 0.5365 - val_accuracy: 0.8760\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.1047 - accuracy: 0.9673 - val_loss: 0.5401 - val_accuracy: 0.8756\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1023 - accuracy: 0.9681 - val_loss: 0.5456 - val_accuracy: 0.8754\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0996 - accuracy: 0.9688 - val_loss: 0.5544 - val_accuracy: 0.8746\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 50s 400ms/step - loss: 0.0979 - accuracy: 0.9693 - val_loss: 0.5568 - val_accuracy: 0.8745\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.0952 - accuracy: 0.9702 - val_loss: 0.5585 - val_accuracy: 0.8741\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.0934 - accuracy: 0.9706 - val_loss: 0.5658 - val_accuracy: 0.8742\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.0911 - accuracy: 0.9712 - val_loss: 0.5762 - val_accuracy: 0.8739\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0897 - accuracy: 0.9719 - val_loss: 0.5743 - val_accuracy: 0.8741\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0871 - accuracy: 0.9726 - val_loss: 0.5767 - val_accuracy: 0.8736\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0848 - accuracy: 0.9731 - val_loss: 0.5832 - val_accuracy: 0.8744\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 51s 412ms/step - loss: 0.0822 - accuracy: 0.9739 - val_loss: 0.5839 - val_accuracy: 0.8743\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 50s 404ms/step - loss: 0.0807 - accuracy: 0.9745 - val_loss: 0.5951 - val_accuracy: 0.8749\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.0799 - accuracy: 0.9743 - val_loss: 0.5998 - val_accuracy: 0.8739\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 51s 404ms/step - loss: 0.0773 - accuracy: 0.9755 - val_loss: 0.6033 - val_accuracy: 0.8740\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0761 - accuracy: 0.9754 - val_loss: 0.6065 - val_accuracy: 0.8734\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0756 - accuracy: 0.9759 - val_loss: 0.6123 - val_accuracy: 0.8736\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0727 - accuracy: 0.9765 - val_loss: 0.6161 - val_accuracy: 0.8738\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 50s 404ms/step - loss: 0.0715 - accuracy: 0.9770 - val_loss: 0.6196 - val_accuracy: 0.8726\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 50s 401ms/step - loss: 0.0694 - accuracy: 0.9777 - val_loss: 0.6235 - val_accuracy: 0.8740\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0686 - accuracy: 0.9778 - val_loss: 0.6283 - val_accuracy: 0.8736\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.0693 - accuracy: 0.9774 - val_loss: 0.6359 - val_accuracy: 0.8730\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.0655 - accuracy: 0.9785 - val_loss: 0.6336 - val_accuracy: 0.8731\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.0638 - accuracy: 0.9792 - val_loss: 0.6404 - val_accuracy: 0.8735\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 51s 406ms/step - loss: 0.0623 - accuracy: 0.9798 - val_loss: 0.6492 - val_accuracy: 0.8729\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 51s 404ms/step - loss: 0.0615 - accuracy: 0.9798 - val_loss: 0.6485 - val_accuracy: 0.8735\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0603 - accuracy: 0.9800 - val_loss: 0.6528 - val_accuracy: 0.8726\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 50s 404ms/step - loss: 0.0590 - accuracy: 0.9805 - val_loss: 0.6627 - val_accuracy: 0.8722\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 51s 405ms/step - loss: 0.0579 - accuracy: 0.9809 - val_loss: 0.6666 - val_accuracy: 0.8723\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 50s 403ms/step - loss: 0.0577 - accuracy: 0.9810 - val_loss: 0.6654 - val_accuracy: 0.8720\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0597 - accuracy: 0.9801 - val_loss: 0.6683 - val_accuracy: 0.8724\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.0561 - accuracy: 0.9814 - val_loss: 0.6804 - val_accuracy: 0.8711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1720de97c48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fra_eng.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
